<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

    
    <title>第4章  Spark SQL结构化数据文件处理 | 锦鲤未离</title>

    <meta name="description" content="&lt;p&gt;本章目的在于如何使用Spark SQL模块来处理结构化数据，结构化数据即以&lt;strong&gt;关系型数据库表&lt;/strong&gt;形式管理的数据。&lt;/p&gt;">
    <meta name="keywords" content="">

    

    <meta property="og:locale" content="zh-CN" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content= "第4章  Spark SQL结构化数据文件处理 | 锦鲤未离"  />
    <meta property="og:description" content= "&lt;p&gt;本章目的在于如何使用Spark SQL模块来处理结构化数据，结构化数据即以&lt;strong&gt;关系型数据库表&lt;/strong&gt;形式管理的数据。&lt;/p&gt;" />
    <meta property="og:url" content="http://example.com/posts/spark_04/index.html" />
    <meta property="og:site_name" content="" />
    <meta property="article:author" content="Koi_NL" />
    <meta property="article:publisher" content="" />
    <meta property="og:description" content="&lt;p&gt;本章目的在于如何使用Spark SQL模块来处理结构化数据，结构化数据即以&lt;strong&gt;关系型数据库表&lt;/strong&gt;形式管理的数据。&lt;/p&gt;" />
    <meta name="twitter:title" content="第4章  Spark SQL结构化数据文件处理 | 锦鲤未离"/>
    <meta name="twitter:description" content="&lt;p&gt;本章目的在于如何使用Spark SQL模块来处理结构化数据，结构化数据即以&lt;strong&gt;关系型数据库表&lt;/strong&gt;形式管理的数据。&lt;/p&gt;"/>
    <script type="application/ld+json">
        {
            "description": "&lt;p&gt;本章目的在于如何使用Spark SQL模块来处理结构化数据，结构化数据即以&lt;strong&gt;关系型数据库表&lt;/strong&gt;形式管理的数据。&lt;/p&gt;",
            "author": { "@type": "Person", "name": "Koi_NL" },
            "@type": "BlogPosting",
            "url": "http://example.com/posts/spark_04/index.html",
            "publisher": {
            "@type": "Organization",
            "logo": {
                "@type": "ImageObject",
                "url": "http://example.comundefined"
            },
            "name": "Koi_NL"
            },
            "headline": "第4章  Spark SQL结构化数据文件处理 | 锦鲤未离",
            "datePublished": "2023-03-31T03:48:40.204Z",
            "mainEntityOfPage": {
                "@type": "WebPage",
                "@id": "http://example.com/posts/spark_04/index.html"
            },
            "@context": "http://schema.org"
        }
    </script>




    

    

    

    

    

    

    

    
<link rel="stylesheet" href="/dist/build.css?v=1654266144177.css">


    
<link rel="stylesheet" href="/dist/custom.css?v=1654266144177.css">


    <script>
        window.isPost = true
        window.aomori = {
            
            
            
        }
        window.aomori_logo_typed_animated = false
        window.aomori_search_algolia = false

    </script>

<meta name="generator" content="Hexo 6.1.0"></head>

<body>

    <div class="container">
    <header class="header">
        <div class="header-type">
            
            <div class="header-type-inner">
                
                    <a class="header-type-title" href="/">锦鲤未离</a>
                
    
                
            </div>
        </div>
        <div class="header-menu">
            <div class="header-menu-inner">
                
            </div>
            <div class="header-menu-social">
                
            </div>
        </div>

        <div class="header-menu-mobile">
            <div class="header-menu-mobile-inner" id="mobile-menu-open">
                <i class="icon icon-menu"></i>
            </div>
        </div>
    </header>

    <div class="header-menu-mobile-menu">
        <div class="header-menu-mobile-menu-bg"></div>
        <div class="header-menu-mobile-menu-wrap">
            <div class="header-menu-mobile-menu-inner">
                <div class="header-menu-mobile-menu-close" id="mobile-menu-close">
                    <i class="icon icon-cross"></i>
                </div>
                <div class="header-menu-mobile-menu-list">
                    
                </div>
            </div>
        </div>
    </div>

</div>

    <div class="container">
        <div class="main">
            <section class="inner">
                <section class="inner-main">
                    <div class="post">
    <article id="post-clfz7mlkz00025o1016cu091k" class="article article-type-post" itemscope
    itemprop="blogPost">

    <div class="article-inner">

        
          
        
        
        

        
        <header class="article-header">
            
  
    <h1 class="article-title" itemprop="name">
      第4章  Spark SQL结构化数据文件处理
    </h1>
  

        </header>
        

        <div class="article-more-info article-more-info-post hairline">

            <div class="article-date">
  <time datetime="2023-03-31T03:48:40.204Z" itemprop="datePublished">2023-03-31</time>
</div>

            
            <div class="article-category">
                <a class="article-category-link" href="/categories/spark/">spark</a>
            </div>
            

            

            

        </div>

        <div class="article-entry post-inner-html hairline" itemprop="articleBody">
            <p>本章目的在于如何使用Spark SQL模块来处理结构化数据，结构化数据即以<strong>关系型数据库表</strong>形式管理的数据。</p>
<span id="more"></span>

<p>首先，本章讲解在Linux系统下对结构化数据的处理。Spark SQL模块让用户可以通过<strong>SQL</strong>、<strong>DataFrame API</strong> 和 **DataSet API **三种方式来实现对结构化数据的处理。<br>在Spark-Shell操作下，RDD很容易就能转换成为DataFrame。那么在Windows系统下呢？又该如何使RDD转换成为DataFrame？</p>
<p>启动Spark-Shell的命令如下：<code>$ spark-shell --master local[2]</code>；如不能正常运行，可跳转到spark安装目录，使用命令 <code>$ bin/spark-shell</code> 启动Spark-shell。</p>
<details ><summary>文件：no4-createDataFrame.txt，该文件用于本章例子</summary><div class="fold-content"><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将要用到的文件no4-createDataFrame.txt</span></span><br><span class="line">[root@hadoop01 ~]# hdfs dfs -cat /sparktest/data/no4-createDataFrame.txt</span><br><span class="line">zhangsan 20</span><br><span class="line">lisi 29</span><br><span class="line">wangwu 25</span><br><span class="line">zhaoliu 30</span><br><span class="line">tianqi 35</span><br><span class="line">jerry 40</span><br></pre></td></tr></table></figure></div></details>

<h2 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h2><h3 id="DataFrame的创建"><a href="#DataFrame的创建" class="headerlink" title="DataFrame的创建"></a>DataFrame的创建</h3><p>创建DataFrame的方式是从一个已经存在的RDD调用toDF()方法进行转换，得到DataFrame；或者通过Spark读取数据源直接创建。<br>这里提供两种成员函数printSchema()和show()，作用分别为打印当前对象的Schema元数据信息和结果数据。</p>
<details ><summary>创建DataFrame的两种方式及部分成员函数</summary><div class="fold-content"><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"># 通过文件直接创建DataFrame。除了读取text(.txt)文件，还可以读取csv、json、parquet等 | 创建方式1</span><br><span class="line">scala&gt; val personDF=spark.read.text(&quot;/sparktest/data/no4-createDataFrame.txt&quot;)</span><br><span class="line">personDF: org.apache.spark.sql.DataFrame = [value: string]</span><br><span class="line"></span><br><span class="line"># 打印当前对象的Schema元数据信息：String数据类型，且可为空。</span><br><span class="line">scala&gt; personDF.printSchema()</span><br><span class="line">root</span><br><span class="line"> |-- value: string (nullable = true)</span><br><span class="line"></span><br><span class="line"># 打印当前DataFrame的结果数据</span><br><span class="line">scala&gt; personDF.show()</span><br><span class="line">+-----------+       </span><br><span class="line">|      value    |</span><br><span class="line">+-----------+</span><br><span class="line">|zhangsan 20|</span><br><span class="line">|    lisi 29|</span><br><span class="line">|  wangwu 25|</span><br><span class="line">| zhaoliu 30|</span><br><span class="line">|  tianqi 35|</span><br><span class="line">|   jerry 40|</span><br><span class="line">+-----------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 从已经存在的RDD进行转换得到DataFrame，首先获取数据</span><br><span class="line"># 第一步</span><br><span class="line">scala&gt; case class Person(name:String,age:Int)</span><br><span class="line">defined class Person</span><br><span class="line"># 以上三步可直接写成：</span><br><span class="line">scala&gt; val pDF=sc.textFile(&quot;/sparktest/data/no4-createDataFrame.txt&quot;).map(_.split(&quot; &quot;)).map(x =&gt; Person(x(0),x(1).toInt)).toDF()</span><br><span class="line">scala&gt; pDF.show</span><br><span class="line">+--------+---+      </span><br><span class="line">|    name|age|</span><br><span class="line">+--------+---+</span><br><span class="line">|zhangsan| 20|</span><br><span class="line">|    lisi| 29|</span><br><span class="line">|  wangwu| 25|</span><br><span class="line">| zhaoliu| 30|</span><br><span class="line">|  tianqi| 35|</span><br><span class="line">|   jerry| 40|</span><br><span class="line">+--------+---+</span><br></pre></td></tr></table></figure></div></details>

<h3 id="DataFrame的常用操作"><a href="#DataFrame的常用操作" class="headerlink" title="DataFrame的常用操作"></a>DataFrame的常用操作</h3><p>DataFrame提供了两种语法风格，分别为DSL风格操作和SQL风格操作。对应的就是DataFrame API 、SQL两种方式。<br>DSL风格</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#查看name 字段的数据，PersonDF会随变量名的变化而变化。</span><br><span class="line">#以下五种方式可同义替代。但第四种方法不推荐！原因是因为某些代码可能不支持！</span><br><span class="line">pDF.select(pDF.col(&quot;name&quot;),pDF.col(&quot;age&quot;)).show</span><br><span class="line">pDF.select(pDF(&quot;name&quot;),pDF(&quot;age&quot;)).show</span><br><span class="line">pDF.select(col(&quot;name&quot;),col(&quot;age&quot;)).show</span><br><span class="line">pDF.select(&quot;name&quot;,&quot;age&quot;).show</span><br><span class="line"># pDF.select(&quot;name&quot;,&quot;age&quot;+1).show 不可运行！</span><br><span class="line"># 但其他四个可运行！例如“$&quot;age&quot;+1”、“col(&quot;age&quot;)+1”</span><br><span class="line"># 请注意可以通过as来重命名列名</span><br><span class="line">pDF.select($&quot;name&quot;,($&quot;age&quot;+1).as(&quot;new_age&quot;)).show</span><br><span class="line"></span><br><span class="line">#条件过滤（对应Where语句）</span><br><span class="line">pDF.filter($&quot;age&quot; &gt;30).show</span><br><span class="line"></span><br><span class="line">#分组（对应Group By语句）</span><br><span class="line">pDF.groupBy(&quot;age&quot;).count().show</span><br><span class="line"></span><br><span class="line">#排序（对应Order By语句）</span><br><span class="line">pDF.sort($&quot;age&quot;.desc).show</span><br></pre></td></tr></table></figure>
<h4 id="SQL风格操作"><a href="#SQL风格操作" class="headerlink" title="SQL风格操作"></a>SQL风格操作</h4><p>将DataFrame注册成一个临时表就可以进行SQL风格操作。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; pDF.registerTempTable(&quot;t_koinl&quot;)</span><br><span class="line">scala&gt; spark.sql(&quot;select name, age + 1 from t_koinl&quot;).show</span><br></pre></td></tr></table></figure>
<h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><h2 id="RDD转换为DataFrame"><a href="#RDD转换为DataFrame" class="headerlink" title="RDD转换为DataFrame"></a>RDD转换为DataFrame</h2><p>在上文中，有说明在Spark-shell中如何将RDD转换为DataFrame，在本小节中，来说明如何在Windows系统下开发Scala代码。</p>
<p>一般情况下，可以使用两种方法来实现。第一种方法是利用反射机制来推断包含特定类型对象的Schema。当<strong>case类不能提前定义，即未知数据结构</strong>时，应通过编程接口构造一个Schema，并将其应用在已知的RDD数据中。 </p>
<h2 id="Spark-SQL操作数据源"><a href="#Spark-SQL操作数据源" class="headerlink" title="Spark SQL操作数据源"></a>Spark SQL操作数据源</h2><h3 id="操作MySQL"><a href="#操作MySQL" class="headerlink" title="操作MySQL"></a>操作MySQL</h3><h4 id="读取MySQL种数据"><a href="#读取MySQL种数据" class="headerlink" title="读取MySQL种数据"></a>读取MySQL种数据</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Properties</span><br><span class="line">import org.apache.spark.sql.&#123;DataFrame, SaveMode,SparkSession&#125;</span><br><span class="line"> </span><br><span class="line">object sparkSqlMysql &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    //创建sparkSession对象</span><br><span class="line">    val spark: SparkSession = SparkSession.builder()</span><br><span class="line">      .appName(&quot;sparkSqlMysql&quot;)</span><br><span class="line">      .master(&quot;local[2]&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    //创建Properties对象，配置连接mysql的用户名和密码</span><br><span class="line">    val prop: prop =new Properties()</span><br><span class="line">    prop.setProperty(&quot;user&quot;,&quot;root&quot;)</span><br><span class="line">    prop.setProperty(&quot;password&quot;,&quot;123456&quot;)</span><br><span class="line">    //从数据库里读取数据</span><br><span class="line">    val mysqlDF: DataFrame = spark.read.jdbc(&quot;jdbc:mysql://127.0.0.1:3306/spark&quot;, &quot;person&quot;, prop)</span><br><span class="line">    //显示Mysql中表数据</span><br><span class="line">    mysqlDF.show()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="写入数据到MySQL"><a href="#写入数据到MySQL" class="headerlink" title="写入数据到MySQL"></a>写入数据到MySQL</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Properties</span><br><span class="line">import org.apache.spark.rdd.RDD</span><br><span class="line">import org.apache.spark.sql.&#123;DataFrame, SaveMode,SparkSession&#125;</span><br><span class="line"> case class Person(id:Int,name:String,age:Int)</span><br><span class="line"></span><br><span class="line">object sparkSqlMysql &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    //创建sparkSession对象</span><br><span class="line">    val spark: SparkSession = SparkSession.builder()</span><br><span class="line">      .appName(&quot;sparkSqlMysql&quot;)</span><br><span class="line">      .master(&quot;local[2]&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    val sc = spark.sparkContext.parallelize(Array(&quot;3,wangwu,22&quot;,&quot;4,zhaoliu,26&quot;))</span><br><span class="line">    //切分读取数据</span><br><span class="line">    val data: RDD[Array[String]] = sc.map(_.split(&quot;,&quot;))</span><br><span class="line">    //RDD关联Person</span><br><span class="line">    val personRdd: RDD[Person] = data.map(x =&gt; Person(x(0)。toInt,x(1), x(2).toLong))</span><br><span class="line">    //导入隐式转换</span><br><span class="line">    import spark.implicits._</span><br><span class="line">    //将RDD转换成DataFrame</span><br><span class="line">    val personDF: DataFrame = personRdd.toDF()</span><br><span class="line">    //创建Properties对象，配置连接mysql的用户名和密码</span><br><span class="line">    val prop =new Properties()</span><br><span class="line">    prop.setProperty(&quot;user&quot;,&quot;root&quot;)</span><br><span class="line">    prop.setProperty(&quot;password&quot;,&quot;123456&quot;)</span><br><span class="line">    //将personDF写入MySQL</span><br><span class="line"> </span><br><span class="line">    personDF.write.mode(SaveMode.Append).jdbc(&quot;jdbc:mysql://127.0.0.1:3306/spark?useUnicode=true&amp;characterEncoding=utf8&quot;,&quot;person&quot;,prop)</span><br><span class="line">    personDF.show()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="操作Hive"><a href="#操作Hive" class="headerlink" title="操作Hive"></a>操作Hive</h3>
        </div>

    </div>

    

    

    

    

    

    
<nav class="article-nav">
  
    <a href="/posts/spark_05/" id="article-nav-newer" class="article-nav-link-wrap">
      <div class="article-nav-caption">下一篇</div>
      <div class="article-nav-title">
        
          第5章  HBase 分布式数据库
        
      </div>
    </a>
  
  
    <a href="/posts/spark_01/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-caption">上一篇</div>
      <div class="article-nav-title">第1章  Scala语言基础.</div>
    </a>
  
</nav>


    <section class="share">
        <div class="share-title">分享</div>
        <a class="share-item" target="_blank"
            href="https://twitter.com/share?text=第4章  Spark SQL结构化数据文件处理 - 锦鲤未离&url=http%3A%2F%2Fexample.com%2Fposts%2Fspark_04%2F">
            <ion-icon name="logo-twitter"></ion-icon>
        </a>
        <a class="share-item" target="_blank"
            href="https://www.facebook.com/sharer.php?title=第4章  Spark SQL结构化数据文件处理 - 锦鲤未离&u=http%3A%2F%2Fexample.com%2Fposts%2Fspark_04%2F">
            <ion-icon name="logo-facebook"></ion-icon>
        </a>
        <!-- <a class="share-item" target="_blank"
            href="https://service.weibo.com/share/share.php?title=第4章  Spark SQL结构化数据文件处理 - 锦鲤未离&url=http://example.com/posts/spark_04/&pic=">
            <div class="n-icon n-icon-weibo"></div>
        </a> -->
    </section>

</article>
















</div>
                </section>
            </section>

            
            <aside class="sidebar ">
                


<div class="widget" id="widget">
    
      
  <div class="widget-wrap">
    <div class="widget-inner">
      <div class="toc post-toc-html"></div>
    </div>
  </div>

    
      
  <div class="widget-wrap widget-cate">
    <div class="widget-title"><span>Categories</span></div>
    <div class="widget-inner">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Applied-Statistics/">Applied Statistics</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Data-Structure-and-Algorithm/">Data Structure and Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Data-Mining/">Data-Mining</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Data-Processing-and-Analysing/">Data-Processing-and-Analysing</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hadoop/">Hadoop</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/MATLAB/">MATLAB</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/SqlServer/">SqlServer</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/java/">java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/spark/">spark</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E5%88%86%E6%9E%90/">时间序列分析</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B5%8B%E8%AF%95%E5%8C%BA/">测试区</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">深度卷积神经网络</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%B5%E5%AD%90%E8%AE%BE%E5%A4%87/">电子设备</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%80%83%E8%AF%95/">考试</a></li></ul>
    </div>
  </div>


    
      

    
      
  <div class="widget-wrap widget-recent-posts">
    <div class="widget-title"><span>Recent Posts</span></div>
    <div class="widget-inner">
      <ul>
        
          <li>
            <a href="/posts/spark_05/">第5章  HBase 分布式数据库</a>
          </li>
        
          <li>
            <a href="/posts/spark_04/">第4章  Spark SQL结构化数据文件处理</a>
          </li>
        
          <li>
            <a href="/posts/spark_01/">第1章  Scala语言基础.</a>
          </li>
        
          <li>
            <a href="/posts/spark_03/">第3章  Spark RDD弹性分布式数据集</a>
          </li>
        
          <li>
            <a href="/posts/spark_08/">第8章  Spark MLlib机器学习算法库</a>
          </li>
        
      </ul>
    </div>
  </div>

    
      
  <div class="widget-wrap widget-archive">
    <div class="widget-title"><span>Archive</span></div>
    <div class="widget-inner">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/">2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/">2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1999/">1999</a></li></ul>
    </div>
  </div>


    
</div>

<div id="backtop"><i class="icon icon-arrow-up"></i></div>
            </aside>
            
        </div>
    </div>

    <footer class="footer">
    <div class="footer-wave">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1440 320"><path fill="#3c4859" fill-opacity="1" d="M0,160L60,181.3C120,203,240,245,360,240C480,235,600,181,720,186.7C840,192,960,256,1080,261.3C1200,267,1320,213,1380,186.7L1440,160L1440,320L1380,320C1320,320,1200,320,1080,320C960,320,840,320,720,320C600,320,480,320,360,320C240,320,120,320,60,320L0,320Z"></path></svg>
    </div>

    <!-- Please do not remove this -->
    <!-- 开源不易，请勿删除 -->
    <div class="footer-wrap">
        <div class="footer-inner"> 
            锦鲤未离 &copy; 2023<br>
            Powered By Hexo · Theme By <a href="https://linhong.me/" target="_blank">Aomori</a> · <a href="https://github.com/lh1me/hexo-theme-aomori" target="_blank">Github</a>
        </div>
    </div>

</footer>

<script type="module" src="https://unpkg.com/ionicons@6.0.2/dist/ionicons/ionicons.esm.js"></script>






<script src="/dist/build.js?1654266144177.js"></script>


<script src="/dist/custom.js?1654266144177.js"></script>













</body>

</html>