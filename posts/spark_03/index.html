<!DOCTYPE html>
<html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init">
<head>
  <meta charset="utf-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

  <link rel="icon" href="/favicon.ico">
  
  <title>锦鲤未离 | 第3章  Spark RDD弹性分布式数据集</title>
  
<link rel="stylesheet" href="/css/style.css">

  
<link rel="stylesheet" href="/lib/fancybox/jquery.fancybox-1.3.4.css">

  <!--在这里倒入jquery 方便处理部分页面的jquery-->
  <script src="https://cdn.staticfile.org/jquery/1.7/jquery.min.js" type="text/javascript" ></script>
<meta name="generator" content="Hexo 6.1.0"></head>

<body>
	<header class="site-header navfixed-false">
  <div class="container">
      <h1><a href="/" title="锦鲤未离"><span class="octicon octicon-mark-github"></span> 锦鲤未离</a></h1>
      <nav class="site-header-nav" role="navigation">
        
              
              <a href="/"  class=" site-header-nav-item hvr-underline-from-center" title="Home">Home</a>
        
              
              <a href="/categories/"  class=" site-header-nav-item hvr-underline-from-center" title="Category">Category</a>
        
              
              <a href="/open-source/"  class=" site-header-nav-item hvr-underline-from-center" title="Open-Source">Open-Source</a>
        
              
              <a href="/message/"  class=" site-header-nav-item hvr-underline-from-center" title="Message">Message</a>
        
      </nav>
  </div>
</header>

	
<section class="collection-head geopattern" data-pattern-id="第3章  Spark RDD弹性分布式数据集" >
    <div class="container">
        <div class="collection-title">
            <h1 class="collection-header">
                第3章  Spark RDD弹性分布式数据集
            </h1>
            
                <div class="collection-info">
                    <span class="meta-info">
                        <span class="octicon octicon-calendar"></span>
                        <time datetime="2023-03-19T02:50:11.581Z" itemprop="datePublished">2023-03-19</time>
                    </span>
                    
                        <span class="meta-info">
                            <span class="octicon octicon-file-directory"></span>
                            <a href='/categories/spark/' title=''>spark</a>
                        </span>
                    
                </div>
            
        </div>
    </div>
</section>
	<section class="container">
    <div class="columns">
        <div class="column  three-fourths " >
            <article class="article-content markdown-body">
                <p>RDD即弹性分布式数据集，是一个数据结构。其是分布式内存的抽象概念，容错且并行。</p>
<span id="more"></span>
<p>请在spark目录使用<code>bin/spark-shell --master local[2]</code> 命令来编写程序。</p>
<p>首先，本章介绍了RDD的三种创建方式；<br>其次，介绍了RDD的两种操作，并以Shell RDD实现词频统计的案例来演示操作；<br>最后，简单介绍了RDD的相关知识，包括RDD的分区方式、RDD的依赖关系、RDD的容错方式以及Spark的任务调度。</p>
<details ><summary>open 本章所用到的文件RddTest.txt</summary><div class="fold-content"><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在Linux本地创建一个需要进行词频统计的文件</span></span><br><span class="line">[root@hadoop01 ~]# vim /export/data/RddTest.txt    </span><br><span class="line">hadoop spark</span><br><span class="line">itcast rdd</span><br><span class="line">scala spark</span><br><span class="line">spark itcast</span><br><span class="line">itcast hadoop</span><br></pre></td></tr></table></figure></div></details>
<h2 id="RDD的创建方式"><a href="#RDD的创建方式" class="headerlink" title="RDD的创建方式"></a>RDD的创建方式</h2><p>Spark有三种创建RDD的方式，分别为于<strong>文件系统</strong>（本地或HDFS）和<strong>并行集合</strong>创建RDD。具体操作方式如下：</p>
<details ><summary>创建RDD的两种方式</summary><div class="fold-content"><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">在linux本地读取文件创建RDD</span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">val lines=sc.textFile(<span class="string">&quot;file:///export/data/RddTest.txt&quot;</span>)</span>   </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">假设在HDFS的/data目录下有同样的一个文件</span> </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">scala&gt; val testRDD=sc.textFile(<span class="string">&quot;/data/test.txt&quot;</span>)</span>  </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">从并行集合上面创建RDD</span>  </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">scala&gt; val arrRDD=sc.parallelize(Array(1,2,3,4,5))</span>    </span><br></pre></td></tr></table></figure></div></details>

<h2 id="RDD的处理过程"><a href="#RDD的处理过程" class="headerlink" title="RDD的处理过程"></a>RDD的处理过程</h2><p>RDD采用惰性调用，即真正的计算只在行动算子中。</p>
<h3 id="1-RDD的转换算子"><a href="#1-RDD的转换算子" class="headerlink" title="1. RDD的转换算子"></a>1. RDD的转换算子</h3><p>RDD的转换算子有5个：filter(func) | map(func) | flatMap(func) | groupByKey() | reduceByKey(func)。其作用分别为筛选、拆分行元素、拆分全部元素、合并、合并且聚合。</p>
<ul>
<li>可以将reduceByKey()理解为groupByKey()，而当reduceByKey(func)时，func的作用则千变万化。</li>
</ul>
<details ><summary>RDD的转换算子举例</summary><div class="fold-content"><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">筛选出满足条件的元素，该行含有hadoop的元素：<span class="string">&quot;hadoop spark&quot;</span> <span class="string">&quot;itcast hadoop&quot;</span></span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">val linesWithSpark=lines.filter(line=&gt;line.contains(<span class="string">&quot;hadoop&quot;</span>))</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">文件的每一行内容都拆分成一行行的单词元素：Array(<span class="string">&quot;hadoop&quot;</span>,<span class="string">&quot;spark&quot;</span>) Array(<span class="string">&quot;itcast&quot;</span>,<span class="string">&quot;rdd&quot;</span>) ...</span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">val words=lines.map(line=&gt;line.split(<span class="string">&quot; &quot;</span>))</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">文件的每一行内容都拆分成一个个的单词元素：<span class="string">&quot;hadoop&quot;</span> <span class="string">&quot;spark&quot;</span> <span class="string">&quot;itcast&quot;</span> <span class="string">&quot;rdd&quot;</span> ...</span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">val words=lines.flatMap(line=&gt;line.split(<span class="string">&quot; &quot;</span>))</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">进行flatMap(func)操作，再进行map()操作，最终呈现例：(<span class="string">&quot;hadoop,1&quot;</span>) (<span class="string">&quot;spark,1&quot;</span>) (<span class="string">&quot;itcast&quot;</span>,1) ...</span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">val words=lines.flatMap(line=&gt;line.split(<span class="string">&quot; &quot;</span>)).map(word=&gt;(word,1))</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">先执行groupByKey()操作，将所有Key相同的Value值合并到一起，有(<span class="string">&quot;spark&quot;</span>,(1,1,1))</span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">val groupWords=words.groupByKey()</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">先生成键值对如(<span class="string">&quot;spark&quot;</span>,(1,1,1))；然后执行函数func操作，即执行(a,b)=&gt;a+b，该函数作用是聚合求和，得到最终结果(<span class="string">&quot;spark&quot;</span>,3)</span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">val reduceWords=words.reduceByKey((a,b)=&gt;a+b)</span></span><br></pre></td></tr></table></figure></div></details>

<h3 id="2-RDD的行动算子"><a href="#2-RDD的行动算子" class="headerlink" title="2. RDD的行动算子"></a>2. RDD的行动算子</h3><p>RDD的行动算子有6个：count() | first() | take(n) | reduce(func) | collect() | foreach(func)，其作用分别为求个数、第一元素、前几元素、聚合元素、转为数组、遍历输出</p>
<details ><summary>RDD的行动算子举例</summary><div class="fold-content"><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">val arrRdd=sc.parallelize(Array(1,2,3,4,5))</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">count() 返回数据集中元素个数</span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">arrRdd.count()</span>    </span><br><span class="line">res0: Long = 5</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">first() 返回数组的第一个元素</span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">arrRdd.first()</span>    </span><br><span class="line">res1: Int = 1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">take(n) 以数组形式返回数组集中前n个元素</span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">arrRdd.take(3)</span>    </span><br><span class="line">res2: Array[Int] = Array(1, 2, 3)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">reduce(func) 聚合数据集中的元素</span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">arrRdd.reduce((a,b)=&gt;a+b)</span>    </span><br><span class="line">res3: Int = 15</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">collect() 以数组的形式返回数据集中的所有元素</span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">arrRdd.collect()</span>   </span><br><span class="line">res4: Array[Int] = Array(1, 2, 3, 4, 5)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">foreach(func) 将数据集中的每个元素传递</span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">arrRdd.foreach(x =&gt; println(x))</span>    </span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td></tr></table></figure></div></details>


<h2 id="RDD实现词频统计"><a href="#RDD实现词频统计" class="headerlink" title="RDD实现词频统计"></a>RDD实现词频统计</h2><details ><summary>准备好数据后编写RDD进行词频统计的代码</summary><div class="fold-content"><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">在linux本地读取文件RddTest.txt</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">将文件的每一行内容都拆分成一个个的单词元素：：<span class="string">&quot;hadoop&quot;</span> <span class="string">&quot;spark&quot;</span> <span class="string">&quot;itcast&quot;</span> <span class="string">&quot;rdd&quot;</span> ...</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将word变成(word,1)，例如：(<span class="string">&quot;hadoop,1&quot;</span>) (<span class="string">&quot;spark,1&quot;</span>) (<span class="string">&quot;itcast&quot;</span>,1) ...</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">通过reduceByKey操作把（Key，Value）键值对类型的RDD，按单词Key将单词出现的次数Value进行聚合</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">foreach()打印输出结果</span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">sc.textFile(<span class="string">&quot;file:///export/data/RddTest.txt&quot;</span>).flatMap(line=&gt;line.split(<span class="string">&quot; &quot;</span>)).map(word=&gt;(word,1)).reduceByKey((a,b)=&gt;a+b).foreach(println)</span></span><br><span class="line">(spark,3)</span><br><span class="line">(scala,1)</span><br><span class="line">(hadoop,2)</span><br><span class="line">(itcast,3)</span><br><span class="line">(rdd,1)</span><br></pre></td></tr></table></figure></div></details>

<h2 id="RDD的相关知识"><a href="#RDD的相关知识" class="headerlink" title="RDD的相关知识"></a>RDD的相关知识</h2><h3 id="RDD的分区方式"><a href="#RDD的分区方式" class="headerlink" title="RDD的分区方式"></a>RDD的分区方式</h3><p>哈希分区和范围分区</p>
<h3 id="RDD的依赖关系"><a href="#RDD的依赖关系" class="headerlink" title="RDD的依赖关系"></a>RDD的依赖关系</h3><p>宽依赖和窄依赖</p>
<h3 id="RDD的容错机制"><a href="#RDD的容错机制" class="headerlink" title="RDD的容错机制"></a>RDD的容错机制</h3><p>容错机制则故障恢复的方式，一般分为<strong>血统方式和设置检查点方式</strong>。</p>
<h3 id="Spark的任务调度"><a href="#Spark的任务调度" class="headerlink" title="Spark的任务调度"></a>Spark的任务调度</h3>
            </article>
            
                <div class="share">
                    <!--开启分享-->
<div class="share-component" data-disabled="google,twitter,facebook" data-description="RDD即弹性分布式数据集，是一个数据结构。其是分布式内..."></div>


<script src="/js/share.min.js"></script>


                </div>    
            

            
            
                
<div class="comments">
    <div id="gitalk-container"></div>
        
<script src="/js/gitalk.js"></script>

        <script>
            var gitalk = new Gitalk({
                clientID: "",
                clientSecret: "",
                repo: '',
                owner: '',
                admin: [''],
                id: decodeURI('/posts/spark_03/'),
            })
        gitalk.render('gitalk-container')
        </script>
</div>

            

        </div>
        <div class="column one-fourth">
            
                
                


<h3>Post Directory</h3>

<div id="post-directory-module">
	<section class="post-directory">
		<p><strong class="toc-title">文章目录</strong></p>
		<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD%E7%9A%84%E5%88%9B%E5%BB%BA%E6%96%B9%E5%BC%8F"><span class="toc-text">RDD的创建方式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD%E7%9A%84%E5%A4%84%E7%90%86%E8%BF%87%E7%A8%8B"><span class="toc-text">RDD的处理过程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-RDD%E7%9A%84%E8%BD%AC%E6%8D%A2%E7%AE%97%E5%AD%90"><span class="toc-text">1. RDD的转换算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-RDD%E7%9A%84%E8%A1%8C%E5%8A%A8%E7%AE%97%E5%AD%90"><span class="toc-text">2. RDD的行动算子</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD%E5%AE%9E%E7%8E%B0%E8%AF%8D%E9%A2%91%E7%BB%9F%E8%AE%A1"><span class="toc-text">RDD实现词频统计</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD%E7%9A%84%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86"><span class="toc-text">RDD的相关知识</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD%E7%9A%84%E5%88%86%E5%8C%BA%E6%96%B9%E5%BC%8F"><span class="toc-text">RDD的分区方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD%E7%9A%84%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB"><span class="toc-text">RDD的依赖关系</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD%E7%9A%84%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6"><span class="toc-text">RDD的容错机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark%E7%9A%84%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6"><span class="toc-text">Spark的任务调度</span></a></li></ol></li></ol>
	</section>
</div>
            
        </div>
    </div>
</section>

<footer class="container">
    <div class="site-footer" role="contentinfo">
        <div class="copyright left mobile-block">
                © 2016
                <span title="yumemor">yumemor</span>
                <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a>
        </div>

        <ul class="site-footer-links right mobile-hidden">
            <li>
                <a href="javascript:window.scrollTo(0,0)" >TOP</a>
            </li>
        </ul>

        <a href="https://github.com/yumemor/hexo-theme-primer" target="_blank" aria-label="view source code">
            <span class="mega-octicon octicon-mark-github" title="GitHub"></span>
        </a>

        <ul class="site-footer-links mobile-hidden">
            
                  
                  <li>
                    <a href="/"  title="Home">Home</a>
                  </li>
            
                  
                  <li>
                    <a href="/categories/"  title="Category">Category</a>
                  </li>
            
                  
                  <li>
                    <a href="/open-source/"  title="Open-Source">Open-Source</a>
                  </li>
            
                  
                  <li>
                    <a href="/message/"  title="Message">Message</a>
                  </li>
            
            <li>
                <a href="/atom.xml">
                    <span class="octicon octicon-rss" style="color:orange;"></span>
                </a>
            </li>
        </ul>
    </div>
</footer>

		
<script src="/js/geopattern.js"></script>

		
<script src="/js/highlight.pack.js"></script>

		
<script src="/lib/fancybox/jquery.fancybox-1.3.4.pack.js"></script>


		
			
<script src="/js/toc.js"></script>

		

		
<script src="/js/index.js"></script>


		 
<script src="/js/popular_repo.js"></script>
 

	</body>
</html>