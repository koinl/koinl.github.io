<!DOCTYPE html>
<html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init">
<head>
  <meta charset="utf-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

  <link rel="icon" href="/favicon.ico">
  
  <title>锦鲤未离 | 第4章  Spark SQL结构化数据文件处理</title>
  
<link rel="stylesheet" href="/css/style.css">

  
<link rel="stylesheet" href="/lib/fancybox/jquery.fancybox-1.3.4.css">

  <!--在这里倒入jquery 方便处理部分页面的jquery-->
  <script src="https://cdn.staticfile.org/jquery/1.7/jquery.min.js" type="text/javascript" ></script>
<meta name="generator" content="Hexo 6.1.0"></head>

<body>
	<header class="site-header navfixed-false">
  <div class="container">
      <h1><a href="/" title="锦鲤未离"><span class="octicon octicon-mark-github"></span> 锦鲤未离</a></h1>
      <nav class="site-header-nav" role="navigation">
        
              
              <a href="/"  class=" site-header-nav-item hvr-underline-from-center" title="Home">Home</a>
        
              
              <a href="/categories/"  class=" site-header-nav-item hvr-underline-from-center" title="Category">Category</a>
        
              
              <a href="/open-source/"  class=" site-header-nav-item hvr-underline-from-center" title="Open-Source">Open-Source</a>
        
              
              <a href="/message/"  class=" site-header-nav-item hvr-underline-from-center" title="Message">Message</a>
        
      </nav>
  </div>
</header>

	
<section class="collection-head geopattern" data-pattern-id="第4章  Spark SQL结构化数据文件处理" >
    <div class="container">
        <div class="collection-title">
            <h1 class="collection-header">
                第4章  Spark SQL结构化数据文件处理
            </h1>
            
                <div class="collection-info">
                    <span class="meta-info">
                        <span class="octicon octicon-calendar"></span>
                        <time datetime="2023-03-31T03:48:40.204Z" itemprop="datePublished">2023-03-31</time>
                    </span>
                    
                        <span class="meta-info">
                            <span class="octicon octicon-file-directory"></span>
                            <a href='/categories/spark/' title=''>spark</a>
                        </span>
                    
                </div>
            
        </div>
    </div>
</section>
	<section class="container">
    <div class="columns">
        <div class="column  three-fourths " >
            <article class="article-content markdown-body">
                <p>本章目的在于如何使用Spark SQL模块来处理结构化数据，结构化数据即以<strong>关系型数据库表</strong>形式管理的数据。</p>
<span id="more"></span>

<p>首先，本章讲解在Linux系统下对结构化数据的处理。Spark SQL模块让用户可以通过<strong>SQL</strong>、<strong>DataFrame API</strong> 和 **DataSet API **三种方式来实现对结构化数据的处理。<br>在Spark-Shell操作下，RDD很容易就能转换成为DataFrame。那么在Windows系统下呢？又该如何使RDD转换成为DataFrame？</p>
<p>启动Spark-Shell的命令如下：<code>$ spark-shell --master local[2]</code>；如不能正常运行，可跳转到spark安装目录，使用命令 <code>$ bin/spark-shell</code> 启动Spark-shell。</p>
<details ><summary>文件：no4-createDataFrame.txt，该文件用于本章例子</summary><div class="fold-content"><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将要用到的文件no4-createDataFrame.txt</span></span><br><span class="line">[root@hadoop01 ~]# hdfs dfs -cat /sparktest/data/no4-createDataFrame.txt</span><br><span class="line">zhangsan 20</span><br><span class="line">lisi 29</span><br><span class="line">wangwu 25</span><br><span class="line">zhaoliu 30</span><br><span class="line">tianqi 35</span><br><span class="line">jerry 40</span><br></pre></td></tr></table></figure></div></details>

<h2 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h2><h3 id="DataFrame的创建"><a href="#DataFrame的创建" class="headerlink" title="DataFrame的创建"></a>DataFrame的创建</h3><p>创建DataFrame的方式是从一个已经存在的RDD调用toDF()方法进行转换，得到DataFrame；或者通过Spark读取数据源直接创建。<br>这里提供两种成员函数printSchema()和show()，作用分别为打印当前对象的Schema元数据信息和结果数据。</p>
<details ><summary>创建DataFrame的两种方式及部分成员函数</summary><div class="fold-content"><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"># 通过文件直接创建DataFrame。除了读取text(.txt)文件，还可以读取csv、json、parquet等 | 创建方式1</span><br><span class="line">scala&gt; val personDF=spark.read.text(&quot;/sparktest/data/no4-createDataFrame.txt&quot;)</span><br><span class="line">personDF: org.apache.spark.sql.DataFrame = [value: string]</span><br><span class="line"></span><br><span class="line"># 打印当前对象的Schema元数据信息：String数据类型，且可为空。</span><br><span class="line">scala&gt; personDF.printSchema()</span><br><span class="line">root</span><br><span class="line"> |-- value: string (nullable = true)</span><br><span class="line"></span><br><span class="line"># 打印当前DataFrame的结果数据</span><br><span class="line">scala&gt; personDF.show()</span><br><span class="line">+-----------+       </span><br><span class="line">|      value    |</span><br><span class="line">+-----------+</span><br><span class="line">|zhangsan 20|</span><br><span class="line">|    lisi 29|</span><br><span class="line">|  wangwu 25|</span><br><span class="line">| zhaoliu 30|</span><br><span class="line">|  tianqi 35|</span><br><span class="line">|   jerry 40|</span><br><span class="line">+-----------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 从已经存在的RDD进行转换得到DataFrame，首先获取数据</span><br><span class="line"># 第一步</span><br><span class="line">scala&gt; case class Person(name:String,age:Int)</span><br><span class="line">defined class Person</span><br><span class="line"># 以上三步可直接写成：</span><br><span class="line">scala&gt; val pDF=sc.textFile(&quot;/sparktest/data/no4-createDataFrame.txt&quot;).map(_.split(&quot; &quot;)).map(x =&gt; Person(x(0),x(1).toInt)).toDF()</span><br><span class="line">scala&gt; pDF.show</span><br><span class="line">+--------+---+      </span><br><span class="line">|    name|age|</span><br><span class="line">+--------+---+</span><br><span class="line">|zhangsan| 20|</span><br><span class="line">|    lisi| 29|</span><br><span class="line">|  wangwu| 25|</span><br><span class="line">| zhaoliu| 30|</span><br><span class="line">|  tianqi| 35|</span><br><span class="line">|   jerry| 40|</span><br><span class="line">+--------+---+</span><br></pre></td></tr></table></figure></div></details>

<h3 id="DataFrame的常用操作"><a href="#DataFrame的常用操作" class="headerlink" title="DataFrame的常用操作"></a>DataFrame的常用操作</h3><p>DataFrame提供了两种语法风格，分别为DSL风格操作和SQL风格操作。对应的就是DataFrame API 、SQL两种方式。<br>DSL风格</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#查看name 字段的数据，PersonDF会随变量名的变化而变化。</span><br><span class="line">#以下五种方式可同义替代。但第四种方法不推荐！原因是因为某些代码可能不支持！</span><br><span class="line">pDF.select(pDF.col(&quot;name&quot;),pDF.col(&quot;age&quot;)).show</span><br><span class="line">pDF.select(pDF(&quot;name&quot;),pDF(&quot;age&quot;)).show</span><br><span class="line">pDF.select(col(&quot;name&quot;),col(&quot;age&quot;)).show</span><br><span class="line">pDF.select(&quot;name&quot;,&quot;age&quot;).show</span><br><span class="line"># pDF.select(&quot;name&quot;,&quot;age&quot;+1).show 不可运行！</span><br><span class="line"># 但其他四个可运行！例如“$&quot;age&quot;+1”、“col(&quot;age&quot;)+1”</span><br><span class="line"># 请注意可以通过as来重命名列名</span><br><span class="line">pDF.select($&quot;name&quot;,($&quot;age&quot;+1).as(&quot;new_age&quot;)).show</span><br><span class="line"></span><br><span class="line">#条件过滤（对应Where语句）</span><br><span class="line">pDF.filter($&quot;age&quot; &gt;30).show</span><br><span class="line"></span><br><span class="line">#分组（对应Group By语句）</span><br><span class="line">pDF.groupBy(&quot;age&quot;).count().show</span><br><span class="line"></span><br><span class="line">#排序（对应Order By语句）</span><br><span class="line">pDF.sort($&quot;age&quot;.desc).show</span><br></pre></td></tr></table></figure>
<h4 id="SQL风格操作"><a href="#SQL风格操作" class="headerlink" title="SQL风格操作"></a>SQL风格操作</h4><p>将DataFrame注册成一个临时表就可以进行SQL风格操作。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; pDF.registerTempTable(&quot;t_koinl&quot;)</span><br><span class="line">scala&gt; spark.sql(&quot;select name, age + 1 from t_koinl&quot;).show</span><br></pre></td></tr></table></figure>
<h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><h2 id="RDD转换为DataFrame"><a href="#RDD转换为DataFrame" class="headerlink" title="RDD转换为DataFrame"></a>RDD转换为DataFrame</h2><p>在上文中，有说明在Spark-shell中如何将RDD转换为DataFrame，在本小节中，来说明如何在Windows系统下开发Scala代码。</p>
<p>一般情况下，可以使用两种方法来实现。第一种方法是利用反射机制来推断包含特定类型对象的Schema。当<strong>case类不能提前定义，即未知数据结构</strong>时，应通过编程接口构造一个Schema，并将其应用在已知的RDD数据中。 </p>
<h2 id="Spark-SQL操作数据源"><a href="#Spark-SQL操作数据源" class="headerlink" title="Spark SQL操作数据源"></a>Spark SQL操作数据源</h2><h3 id="操作MySQL"><a href="#操作MySQL" class="headerlink" title="操作MySQL"></a>操作MySQL</h3><h4 id="读取MySQL种数据"><a href="#读取MySQL种数据" class="headerlink" title="读取MySQL种数据"></a>读取MySQL种数据</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Properties</span><br><span class="line">import org.apache.spark.sql.&#123;DataFrame, SaveMode,SparkSession&#125;</span><br><span class="line"> </span><br><span class="line">object sparkSqlMysql &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    //创建sparkSession对象</span><br><span class="line">    val spark: SparkSession = SparkSession.builder()</span><br><span class="line">      .appName(&quot;sparkSqlMysql&quot;)</span><br><span class="line">      .master(&quot;local[2]&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    //创建Properties对象，配置连接mysql的用户名和密码</span><br><span class="line">    val prop: prop =new Properties()</span><br><span class="line">    prop.setProperty(&quot;user&quot;,&quot;root&quot;)</span><br><span class="line">    prop.setProperty(&quot;password&quot;,&quot;123456&quot;)</span><br><span class="line">    //从数据库里读取数据</span><br><span class="line">    val mysqlDF: DataFrame = spark.read.jdbc(&quot;jdbc:mysql://127.0.0.1:3306/spark&quot;, &quot;person&quot;, prop)</span><br><span class="line">    //显示Mysql中表数据</span><br><span class="line">    mysqlDF.show()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="写入数据到MySQL"><a href="#写入数据到MySQL" class="headerlink" title="写入数据到MySQL"></a>写入数据到MySQL</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Properties</span><br><span class="line">import org.apache.spark.rdd.RDD</span><br><span class="line">import org.apache.spark.sql.&#123;DataFrame, SaveMode,SparkSession&#125;</span><br><span class="line"> case class Person(id:Int,name:String,age:Int)</span><br><span class="line"></span><br><span class="line">object sparkSqlMysql &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    //创建sparkSession对象</span><br><span class="line">    val spark: SparkSession = SparkSession.builder()</span><br><span class="line">      .appName(&quot;sparkSqlMysql&quot;)</span><br><span class="line">      .master(&quot;local[2]&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    val sc = spark.sparkContext.parallelize(Array(&quot;3,wangwu,22&quot;,&quot;4,zhaoliu,26&quot;))</span><br><span class="line">    //切分读取数据</span><br><span class="line">    val data: RDD[Array[String]] = sc.map(_.split(&quot;,&quot;))</span><br><span class="line">    //RDD关联Person</span><br><span class="line">    val personRdd: RDD[Person] = data.map(x =&gt; Person(x(0)。toInt,x(1), x(2).toLong))</span><br><span class="line">    //导入隐式转换</span><br><span class="line">    import spark.implicits._</span><br><span class="line">    //将RDD转换成DataFrame</span><br><span class="line">    val personDF: DataFrame = personRdd.toDF()</span><br><span class="line">    //创建Properties对象，配置连接mysql的用户名和密码</span><br><span class="line">    val prop =new Properties()</span><br><span class="line">    prop.setProperty(&quot;user&quot;,&quot;root&quot;)</span><br><span class="line">    prop.setProperty(&quot;password&quot;,&quot;123456&quot;)</span><br><span class="line">    //将personDF写入MySQL</span><br><span class="line"> </span><br><span class="line">    personDF.write.mode(SaveMode.Append).jdbc(&quot;jdbc:mysql://127.0.0.1:3306/spark?useUnicode=true&amp;characterEncoding=utf8&quot;,&quot;person&quot;,prop)</span><br><span class="line">    personDF.show()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="操作Hive"><a href="#操作Hive" class="headerlink" title="操作Hive"></a>操作Hive</h3>
            </article>
            
                <div class="share">
                    <!--开启分享-->
<div class="share-component" data-disabled="google,twitter,facebook" data-description="本章目的在于如何使用Spark SQL模块来处理结构化..."></div>


<script src="/js/share.min.js"></script>


                </div>    
            

            
            
                
<div class="comments">
    <div id="gitalk-container"></div>
        
<script src="/js/gitalk.js"></script>

        <script>
            var gitalk = new Gitalk({
                clientID: "",
                clientSecret: "",
                repo: '',
                owner: '',
                admin: [''],
                id: decodeURI('/posts/spark_04/'),
            })
        gitalk.render('gitalk-container')
        </script>
</div>

            

        </div>
        <div class="column one-fourth">
            
                
                


<h3>Post Directory</h3>

<div id="post-directory-module">
	<section class="post-directory">
		<p><strong class="toc-title">文章目录</strong></p>
		<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#DataFrame"><span class="toc-text">DataFrame</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#DataFrame%E7%9A%84%E5%88%9B%E5%BB%BA"><span class="toc-text">DataFrame的创建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DataFrame%E7%9A%84%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C"><span class="toc-text">DataFrame的常用操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#SQL%E9%A3%8E%E6%A0%BC%E6%93%8D%E4%BD%9C"><span class="toc-text">SQL风格操作</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dataset"><span class="toc-text">Dataset</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD%E8%BD%AC%E6%8D%A2%E4%B8%BADataFrame"><span class="toc-text">RDD转换为DataFrame</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-SQL%E6%93%8D%E4%BD%9C%E6%95%B0%E6%8D%AE%E6%BA%90"><span class="toc-text">Spark SQL操作数据源</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%93%8D%E4%BD%9CMySQL"><span class="toc-text">操作MySQL</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96MySQL%E7%A7%8D%E6%95%B0%E6%8D%AE"><span class="toc-text">读取MySQL种数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%99%E5%85%A5%E6%95%B0%E6%8D%AE%E5%88%B0MySQL"><span class="toc-text">写入数据到MySQL</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%93%8D%E4%BD%9CHive"><span class="toc-text">操作Hive</span></a></li></ol></li></ol>
	</section>
</div>
            
        </div>
    </div>
</section>

<footer class="container">
    <div class="site-footer" role="contentinfo">
        <div class="copyright left mobile-block">
                © 2016
                <span title="yumemor">yumemor</span>
                <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a>
        </div>

        <ul class="site-footer-links right mobile-hidden">
            <li>
                <a href="javascript:window.scrollTo(0,0)" >TOP</a>
            </li>
        </ul>

        <a href="https://github.com/yumemor/hexo-theme-primer" target="_blank" aria-label="view source code">
            <span class="mega-octicon octicon-mark-github" title="GitHub"></span>
        </a>

        <ul class="site-footer-links mobile-hidden">
            
                  
                  <li>
                    <a href="/"  title="Home">Home</a>
                  </li>
            
                  
                  <li>
                    <a href="/categories/"  title="Category">Category</a>
                  </li>
            
                  
                  <li>
                    <a href="/open-source/"  title="Open-Source">Open-Source</a>
                  </li>
            
                  
                  <li>
                    <a href="/message/"  title="Message">Message</a>
                  </li>
            
            <li>
                <a href="/atom.xml">
                    <span class="octicon octicon-rss" style="color:orange;"></span>
                </a>
            </li>
        </ul>
    </div>
</footer>

		
<script src="/js/geopattern.js"></script>

		
<script src="/js/highlight.pack.js"></script>

		
<script src="/lib/fancybox/jquery.fancybox-1.3.4.pack.js"></script>


		
			
<script src="/js/toc.js"></script>

		

		
<script src="/js/index.js"></script>


		 
<script src="/js/popular_repo.js"></script>
 

	</body>
</html>