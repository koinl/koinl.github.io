<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

    
    <title>第3章  Spark RDD弹性分布式数据集 | 锦鲤未离</title>

    <meta name="description" content="&lt;p&gt;RDD即弹性分布式数据集，是一个数据结构。其是分布式内存的抽象概念，容错且并行。&lt;/p&gt;">
    <meta name="keywords" content="">

    

    <meta property="og:locale" content="zh-CN" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content= "第3章  Spark RDD弹性分布式数据集 | 锦鲤未离"  />
    <meta property="og:description" content= "&lt;p&gt;RDD即弹性分布式数据集，是一个数据结构。其是分布式内存的抽象概念，容错且并行。&lt;/p&gt;" />
    <meta property="og:url" content="http://example.com/posts/spark_03/index.html" />
    <meta property="og:site_name" content="" />
    <meta property="article:author" content="Koi_NL" />
    <meta property="article:publisher" content="" />
    <meta property="og:description" content="&lt;p&gt;RDD即弹性分布式数据集，是一个数据结构。其是分布式内存的抽象概念，容错且并行。&lt;/p&gt;" />
    <meta name="twitter:title" content="第3章  Spark RDD弹性分布式数据集 | 锦鲤未离"/>
    <meta name="twitter:description" content="&lt;p&gt;RDD即弹性分布式数据集，是一个数据结构。其是分布式内存的抽象概念，容错且并行。&lt;/p&gt;"/>
    <script type="application/ld+json">
        {
            "description": "&lt;p&gt;RDD即弹性分布式数据集，是一个数据结构。其是分布式内存的抽象概念，容错且并行。&lt;/p&gt;",
            "author": { "@type": "Person", "name": "Koi_NL" },
            "@type": "BlogPosting",
            "url": "http://example.com/posts/spark_03/index.html",
            "publisher": {
            "@type": "Organization",
            "logo": {
                "@type": "ImageObject",
                "url": "http://example.comundefined"
            },
            "name": "Koi_NL"
            },
            "headline": "第3章  Spark RDD弹性分布式数据集 | 锦鲤未离",
            "datePublished": "2023-03-19T02:50:11.581Z",
            "mainEntityOfPage": {
                "@type": "WebPage",
                "@id": "http://example.com/posts/spark_03/index.html"
            },
            "@context": "http://schema.org"
        }
    </script>




    

    

    

    

    

    

    

    
<link rel="stylesheet" href="/dist/build.css?v=1654266144177.css">


    
<link rel="stylesheet" href="/dist/custom.css?v=1654266144177.css">


    <script>
        window.isPost = true
        window.aomori = {
            
            
            
        }
        window.aomori_logo_typed_animated = false
        window.aomori_search_algolia = false

    </script>

<meta name="generator" content="Hexo 6.1.0"></head>

<body>

    <div class="container">
    <header class="header">
        <div class="header-type">
            
            <div class="header-type-inner">
                
                    <a class="header-type-title" href="/">锦鲤未离</a>
                
    
                
            </div>
        </div>
        <div class="header-menu">
            <div class="header-menu-inner">
                
            </div>
            <div class="header-menu-social">
                
            </div>
        </div>

        <div class="header-menu-mobile">
            <div class="header-menu-mobile-inner" id="mobile-menu-open">
                <i class="icon icon-menu"></i>
            </div>
        </div>
    </header>

    <div class="header-menu-mobile-menu">
        <div class="header-menu-mobile-menu-bg"></div>
        <div class="header-menu-mobile-menu-wrap">
            <div class="header-menu-mobile-menu-inner">
                <div class="header-menu-mobile-menu-close" id="mobile-menu-close">
                    <i class="icon icon-cross"></i>
                </div>
                <div class="header-menu-mobile-menu-list">
                    
                </div>
            </div>
        </div>
    </div>

</div>

    <div class="container">
        <div class="main">
            <section class="inner">
                <section class="inner-main">
                    <div class="post">
    <article id="post-clfvy9ugz0000uk101vuj0hwn" class="article article-type-post" itemscope
    itemprop="blogPost">

    <div class="article-inner">

        
          
        
        
        

        
        <header class="article-header">
            
  
    <h1 class="article-title" itemprop="name">
      第3章  Spark RDD弹性分布式数据集
    </h1>
  

        </header>
        

        <div class="article-more-info article-more-info-post hairline">

            <div class="article-date">
  <time datetime="2023-03-19T02:50:11.581Z" itemprop="datePublished">2023-03-19</time>
</div>

            
            <div class="article-category">
                <a class="article-category-link" href="/categories/spark/">spark</a>
            </div>
            

            

            

        </div>

        <div class="article-entry post-inner-html hairline" itemprop="articleBody">
            <p>RDD即弹性分布式数据集，是一个数据结构。其是分布式内存的抽象概念，容错且并行。</p>
<span id="more"></span>
<p>请在spark目录使用<code>bin/spark-shell --master local[2]</code> 命令来编写程序。</p>
<p>首先，本章介绍了RDD的三种创建方式；<br>其次，介绍了RDD的两种操作，并以Shell RDD实现词频统计的案例来演示操作；<br>最后，简单介绍了RDD的相关知识，包括RDD的分区方式、RDD的依赖关系、RDD的容错方式以及Spark的任务调度。</p>
<details ><summary>open 本章所用到的文件RddTest.txt</summary><div class="fold-content"><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在Linux本地创建一个需要进行词频统计的文件</span></span><br><span class="line">[root@hadoop01 ~]# vim /export/data/RddTest.txt    </span><br><span class="line">hadoop spark</span><br><span class="line">itcast rdd</span><br><span class="line">scala spark</span><br><span class="line">spark itcast</span><br><span class="line">itcast hadoop</span><br></pre></td></tr></table></figure></div></details>
<h2 id="RDD的创建方式"><a href="#RDD的创建方式" class="headerlink" title="RDD的创建方式"></a>RDD的创建方式</h2><p>Spark有三种创建RDD的方式，分别为于<strong>文件系统</strong>（本地或HDFS）和<strong>并行集合</strong>创建RDD。具体操作方式如下：</p>
<details ><summary>创建RDD的两种方式</summary><div class="fold-content"><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">在linux本地读取文件创建RDD</span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">val lines=sc.textFile(<span class="string">&quot;file:///export/data/RddTest.txt&quot;</span>)</span>   </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">假设在HDFS的/data目录下有同样的一个文件</span> </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">scala&gt; val testRDD=sc.textFile(<span class="string">&quot;/data/test.txt&quot;</span>)</span>  </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">从并行集合上面创建RDD</span>  </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">scala&gt; val arrRDD=sc.parallelize(Array(1,2,3,4,5))</span>    </span><br></pre></td></tr></table></figure></div></details>

<h2 id="RDD的处理过程"><a href="#RDD的处理过程" class="headerlink" title="RDD的处理过程"></a>RDD的处理过程</h2><p>RDD采用惰性调用，即真正的计算只在行动算子中。</p>
<h3 id="1-RDD的转换算子"><a href="#1-RDD的转换算子" class="headerlink" title="1. RDD的转换算子"></a>1. RDD的转换算子</h3><p>RDD的转换算子有5个：filter(func) | map(func) | flatMap(func) | groupByKey() | reduceByKey(func)。其作用分别为筛选、拆分行元素、拆分全部元素、合并、合并且聚合。</p>
<ul>
<li>可以将reduceByKey()理解为groupByKey()，而当reduceByKey(func)时，func的作用则千变万化。</li>
</ul>
<details ><summary>RDD的转换算子举例</summary><div class="fold-content"><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">筛选出满足条件的元素，该行含有hadoop的元素：<span class="string">&quot;hadoop spark&quot;</span> <span class="string">&quot;itcast hadoop&quot;</span></span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">val linesWithSpark=lines.filter(line=&gt;line.contains(<span class="string">&quot;hadoop&quot;</span>))</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">文件的每一行内容都拆分成一行行的单词元素：Array(<span class="string">&quot;hadoop&quot;</span>,<span class="string">&quot;spark&quot;</span>) Array(<span class="string">&quot;itcast&quot;</span>,<span class="string">&quot;rdd&quot;</span>) ...</span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">val words=lines.map(line=&gt;line.split(<span class="string">&quot; &quot;</span>))</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">文件的每一行内容都拆分成一个个的单词元素：<span class="string">&quot;hadoop&quot;</span> <span class="string">&quot;spark&quot;</span> <span class="string">&quot;itcast&quot;</span> <span class="string">&quot;rdd&quot;</span> ...</span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">val words=lines.flatMap(line=&gt;line.split(<span class="string">&quot; &quot;</span>))</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">进行flatMap(func)操作，再进行map()操作，最终呈现例：(<span class="string">&quot;hadoop,1&quot;</span>) (<span class="string">&quot;spark,1&quot;</span>) (<span class="string">&quot;itcast&quot;</span>,1) ...</span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">val words=lines.flatMap(line=&gt;line.split(<span class="string">&quot; &quot;</span>)).map(word=&gt;(word,1))</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">先执行groupByKey()操作，将所有Key相同的Value值合并到一起，有(<span class="string">&quot;spark&quot;</span>,(1,1,1))</span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">val groupWords=words.groupByKey()</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">先生成键值对如(<span class="string">&quot;spark&quot;</span>,(1,1,1))；然后执行函数func操作，即执行(a,b)=&gt;a+b，该函数作用是聚合求和，得到最终结果(<span class="string">&quot;spark&quot;</span>,3)</span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">val reduceWords=words.reduceByKey((a,b)=&gt;a+b)</span></span><br></pre></td></tr></table></figure></div></details>

<h3 id="2-RDD的行动算子"><a href="#2-RDD的行动算子" class="headerlink" title="2. RDD的行动算子"></a>2. RDD的行动算子</h3><p>RDD的行动算子有6个：count() | first() | take(n) | reduce(func) | collect() | foreach(func)，其作用分别为求个数、第一元素、前几元素、聚合元素、转为数组、遍历输出</p>
<details ><summary>RDD的行动算子举例</summary><div class="fold-content"><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">val arrRdd=sc.parallelize(Array(1,2,3,4,5))</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">count() 返回数据集中元素个数</span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">arrRdd.count()</span>    </span><br><span class="line">res0: Long = 5</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">first() 返回数组的第一个元素</span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">arrRdd.first()</span>    </span><br><span class="line">res1: Int = 1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">take(n) 以数组形式返回数组集中前n个元素</span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">arrRdd.take(3)</span>    </span><br><span class="line">res2: Array[Int] = Array(1, 2, 3)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">reduce(func) 聚合数据集中的元素</span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">arrRdd.reduce((a,b)=&gt;a+b)</span>    </span><br><span class="line">res3: Int = 15</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">collect() 以数组的形式返回数据集中的所有元素</span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">arrRdd.collect()</span>   </span><br><span class="line">res4: Array[Int] = Array(1, 2, 3, 4, 5)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">foreach(func) 将数据集中的每个元素传递</span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">arrRdd.foreach(x =&gt; println(x))</span>    </span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td></tr></table></figure></div></details>


<h2 id="RDD实现词频统计"><a href="#RDD实现词频统计" class="headerlink" title="RDD实现词频统计"></a>RDD实现词频统计</h2><details ><summary>准备好数据后编写RDD进行词频统计的代码</summary><div class="fold-content"><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">在linux本地读取文件RddTest.txt</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">将文件的每一行内容都拆分成一个个的单词元素：：<span class="string">&quot;hadoop&quot;</span> <span class="string">&quot;spark&quot;</span> <span class="string">&quot;itcast&quot;</span> <span class="string">&quot;rdd&quot;</span> ...</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将word变成(word,1)，例如：(<span class="string">&quot;hadoop,1&quot;</span>) (<span class="string">&quot;spark,1&quot;</span>) (<span class="string">&quot;itcast&quot;</span>,1) ...</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">通过reduceByKey操作把（Key，Value）键值对类型的RDD，按单词Key将单词出现的次数Value进行聚合</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">foreach()打印输出结果</span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">sc.textFile(<span class="string">&quot;file:///export/data/RddTest.txt&quot;</span>).flatMap(line=&gt;line.split(<span class="string">&quot; &quot;</span>)).map(word=&gt;(word,1)).reduceByKey((a,b)=&gt;a+b).foreach(println)</span></span><br><span class="line">(spark,3)</span><br><span class="line">(scala,1)</span><br><span class="line">(hadoop,2)</span><br><span class="line">(itcast,3)</span><br><span class="line">(rdd,1)</span><br></pre></td></tr></table></figure></div></details>

<h2 id="RDD的相关知识"><a href="#RDD的相关知识" class="headerlink" title="RDD的相关知识"></a>RDD的相关知识</h2><h3 id="RDD的分区方式"><a href="#RDD的分区方式" class="headerlink" title="RDD的分区方式"></a>RDD的分区方式</h3><p>哈希分区和范围分区</p>
<h3 id="RDD的依赖关系"><a href="#RDD的依赖关系" class="headerlink" title="RDD的依赖关系"></a>RDD的依赖关系</h3><p>宽依赖和窄依赖</p>
<h3 id="RDD的容错机制"><a href="#RDD的容错机制" class="headerlink" title="RDD的容错机制"></a>RDD的容错机制</h3><p>容错机制则故障恢复的方式，一般分为<strong>血统方式和设置检查点方式</strong>。</p>
<h3 id="Spark的任务调度"><a href="#Spark的任务调度" class="headerlink" title="Spark的任务调度"></a>Spark的任务调度</h3>
        </div>

    </div>

    

    

    

    

    

    
<nav class="article-nav">
  
    <a href="/posts/spark_01/" id="article-nav-newer" class="article-nav-link-wrap">
      <div class="article-nav-caption">下一篇</div>
      <div class="article-nav-title">
        
          第1章  Scala语言基础.
        
      </div>
    </a>
  
  
    <a href="/posts/spark_08/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-caption">上一篇</div>
      <div class="article-nav-title">第8章  Spark MLlib机器学习算法库</div>
    </a>
  
</nav>


    <section class="share">
        <div class="share-title">分享</div>
        <a class="share-item" target="_blank"
            href="https://twitter.com/share?text=第3章  Spark RDD弹性分布式数据集 - 锦鲤未离&url=http%3A%2F%2Fexample.com%2Fposts%2Fspark_03%2F">
            <ion-icon name="logo-twitter"></ion-icon>
        </a>
        <a class="share-item" target="_blank"
            href="https://www.facebook.com/sharer.php?title=第3章  Spark RDD弹性分布式数据集 - 锦鲤未离&u=http%3A%2F%2Fexample.com%2Fposts%2Fspark_03%2F">
            <ion-icon name="logo-facebook"></ion-icon>
        </a>
        <!-- <a class="share-item" target="_blank"
            href="https://service.weibo.com/share/share.php?title=第3章  Spark RDD弹性分布式数据集 - 锦鲤未离&url=http://example.com/posts/spark_03/&pic=">
            <div class="n-icon n-icon-weibo"></div>
        </a> -->
    </section>

</article>
















</div>
                </section>
            </section>

            
            <aside class="sidebar ">
                


<div class="widget" id="widget">
    
      
  <div class="widget-wrap">
    <div class="widget-inner">
      <div class="toc post-toc-html"></div>
    </div>
  </div>

    
      
  <div class="widget-wrap widget-cate">
    <div class="widget-title"><span>Categories</span></div>
    <div class="widget-inner">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Applied-Statistics/">Applied Statistics</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Data-Structure-and-Algorithm/">Data Structure and Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Data-Mining/">Data-Mining</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Data-Processing-and-Analysing/">Data-Processing-and-Analysing</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hadoop/">Hadoop</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/MATLAB/">MATLAB</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/SqlServer/">SqlServer</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/java/">java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/spark/">spark</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E5%88%86%E6%9E%90/">时间序列分析</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B5%8B%E8%AF%95%E5%8C%BA/">测试区</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">深度卷积神经网络</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%B5%E5%AD%90%E8%AE%BE%E5%A4%87/">电子设备</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%80%83%E8%AF%95/">考试</a></li></ul>
    </div>
  </div>


    
      

    
      
  <div class="widget-wrap widget-recent-posts">
    <div class="widget-title"><span>Recent Posts</span></div>
    <div class="widget-inner">
      <ul>
        
          <li>
            <a href="/posts/spark_05/">第5章  HBase 分布式数据库</a>
          </li>
        
          <li>
            <a href="/posts/spark_04/">第4章  Spark SQL结构化数据文件处理</a>
          </li>
        
          <li>
            <a href="/posts/spark_01/">第1章  Scala语言基础.</a>
          </li>
        
          <li>
            <a href="/posts/spark_03/">第3章  Spark RDD弹性分布式数据集</a>
          </li>
        
          <li>
            <a href="/posts/spark_08/">第8章  Spark MLlib机器学习算法库</a>
          </li>
        
      </ul>
    </div>
  </div>

    
      
  <div class="widget-wrap widget-archive">
    <div class="widget-title"><span>Archive</span></div>
    <div class="widget-inner">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/">2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/">2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1999/">1999</a></li></ul>
    </div>
  </div>


    
</div>

<div id="backtop"><i class="icon icon-arrow-up"></i></div>
            </aside>
            
        </div>
    </div>

    <footer class="footer">
    <div class="footer-wave">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1440 320"><path fill="#3c4859" fill-opacity="1" d="M0,160L60,181.3C120,203,240,245,360,240C480,235,600,181,720,186.7C840,192,960,256,1080,261.3C1200,267,1320,213,1380,186.7L1440,160L1440,320L1380,320C1320,320,1200,320,1080,320C960,320,840,320,720,320C600,320,480,320,360,320C240,320,120,320,60,320L0,320Z"></path></svg>
    </div>

    <!-- Please do not remove this -->
    <!-- 开源不易，请勿删除 -->
    <div class="footer-wrap">
        <div class="footer-inner"> 
            锦鲤未离 &copy; 2023<br>
            Powered By Hexo · Theme By <a href="https://linhong.me/" target="_blank">Aomori</a> · <a href="https://github.com/lh1me/hexo-theme-aomori" target="_blank">Github</a>
        </div>
    </div>

</footer>

<script type="module" src="https://unpkg.com/ionicons@6.0.2/dist/ionicons/ionicons.esm.js"></script>






<script src="/dist/build.js?1654266144177.js"></script>


<script src="/dist/custom.js?1654266144177.js"></script>













</body>

</html>